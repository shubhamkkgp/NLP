{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7968,"databundleVersionId":828965,"sourceType":"competition"}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-02T09:34:18.072828Z","iopub.execute_input":"2024-07-02T09:34:18.073327Z","iopub.status.idle":"2024-07-02T09:34:18.081015Z","shell.execute_reply.started":"2024-07-02T09:34:18.073292Z","shell.execute_reply":"2024-07-02T09:34:18.080002Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"/kaggle/input/google-quest-challenge/sample_submission.csv\n/kaggle/input/google-quest-challenge/train.csv\n/kaggle/input/google-quest-challenge/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load data\ntrain_df = pd.read_csv('/kaggle/input/google-quest-challenge/train.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:34:20.260247Z","iopub.execute_input":"2024-07-02T09:34:20.260628Z","iopub.status.idle":"2024-07-02T09:34:20.439781Z","shell.execute_reply.started":"2024-07-02T09:34:20.260600Z","shell.execute_reply":"2024-07-02T09:34:20.438697Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/google-quest-challenge/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:34:21.537873Z","iopub.execute_input":"2024-07-02T09:34:21.538856Z","iopub.status.idle":"2024-07-02T09:34:21.561075Z","shell.execute_reply.started":"2024-07-02T09:34:21.538819Z","shell.execute_reply":"2024-07-02T09:34:21.559963Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"import re\ndef remove_url(s):\n  return re.sub(r'http\\S+', '', s)\n\ntrain_df['question_body'] = train_df['question_body'].apply(remove_url)\ntrain_df['question_title'] = train_df['question_title'].apply(remove_url)\ntrain_df['answer'] = train_df['answer'].apply(remove_url)\ntest_df['question_body'] = test_df['question_body'].apply(remove_url)\ntest_df['question_title'] = test_df['question_title'].apply(remove_url)\ntest_df['answer'] = test_df['answer'].apply(remove_url)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:34:22.260203Z","iopub.execute_input":"2024-07-02T09:34:22.260580Z","iopub.status.idle":"2024-07-02T09:34:22.321338Z","shell.execute_reply.started":"2024-07-02T09:34:22.260551Z","shell.execute_reply":"2024-07-02T09:34:22.320443Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"def remove_tag(s):\n  return re.sub(r'<.*?>', ' ', s)\n\ncolumns_to_clean = ['question_body', 'question_title', 'answer']\ntrain_df[columns_to_clean] = train_df[columns_to_clean].apply(lambda x: x.apply(remove_tag))\ntest_df[columns_to_clean] = test_df[columns_to_clean].apply(lambda x: x.apply(remove_tag))","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:34:22.942655Z","iopub.execute_input":"2024-07-02T09:34:22.943029Z","iopub.status.idle":"2024-07-02T09:34:22.997365Z","shell.execute_reply.started":"2024-07-02T09:34:22.942999Z","shell.execute_reply":"2024-07-02T09:34:22.996445Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"def lower_words(s):\n   return s.lower()\n\ntrain_df[columns_to_clean] = train_df[columns_to_clean].apply(lambda x: x.apply(lower_words))\ntest_df[columns_to_clean] = test_df[columns_to_clean].apply(lambda x: x.apply(lower_words))","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:34:23.713938Z","iopub.execute_input":"2024-07-02T09:34:23.714306Z","iopub.status.idle":"2024-07-02T09:34:23.755140Z","shell.execute_reply.started":"2024-07-02T09:34:23.714277Z","shell.execute_reply":"2024-07-02T09:34:23.754037Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"def decontracted(phrase):\n  \"\"\"decontracted takes text and convert contractions into natural form.\n     ref: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python/47091490#47091490\"\"\"\n\n  # specific\n  phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n  phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n  phrase = re.sub(r\"won\\’t\", \"will not\", phrase)\n  phrase = re.sub(r\"can\\’t\", \"can not\", phrase)\n\n  # general\n  phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n  phrase = re.sub(r\"\\'re\", \" are\", phrase)\n  phrase = re.sub(r\"\\'s\", \" is\", phrase)\n  phrase = re.sub(r\"\\'d\", \" would\", phrase)\n  phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n  phrase = re.sub(r\"\\'t\", \" not\", phrase)\n  phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n  phrase = re.sub(r\"\\'m\", \" am\", phrase)\n\n  phrase = re.sub(r\"n\\’t\", \" not\", phrase)\n  phrase = re.sub(r\"\\’re\", \" are\", phrase)\n  phrase = re.sub(r\"\\’s\", \" is\", phrase)\n  phrase = re.sub(r\"\\’d\", \" would\", phrase)\n  phrase = re.sub(r\"\\’ll\", \" will\", phrase)\n  phrase = re.sub(r\"\\’t\", \" not\", phrase)\n  phrase = re.sub(r\"\\’ve\", \" have\", phrase)\n  phrase = re.sub(r\"\\’m\", \" am\", phrase)\n\n  return phrase\n\ntrain_df[columns_to_clean] = train_df[columns_to_clean].apply(lambda x: x.apply(decontracted))\ntest_df[columns_to_clean] = test_df[columns_to_clean].apply(lambda x: x.apply(decontracted))","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:34:24.335102Z","iopub.execute_input":"2024-07-02T09:34:24.336025Z","iopub.status.idle":"2024-07-02T09:34:24.994719Z","shell.execute_reply.started":"2024-07-02T09:34:24.335994Z","shell.execute_reply":"2024-07-02T09:34:24.993792Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"train_df['question_body'][1]","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:34:24.996403Z","iopub.execute_input":"2024-07-02T09:34:24.996696Z","iopub.status.idle":"2024-07-02T09:34:25.003253Z","shell.execute_reply.started":"2024-07-02T09:34:24.996669Z","shell.execute_reply":"2024-07-02T09:34:25.002247Z"},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"'i am trying to understand what kinds of places the spam values on p 231 refer to in the 5th edition main book for shadowrun.\\n\\nper p 15, a sprawl is a plex, a plex is a \"metropolitan complex, short for metroplex\". per google a metroplex is \" a very large metropolitan area, especially one that is an aggregation of two or more cities\".  a city downtown and sprawl downtown would tend to have similar densities, but for some reason the sprawl (which includes suburbs?) has a higher spam zone noise rating (p 231).  similarly, i would think of a downtown as being more dense and noisy (e.g. office buildings and street vendors) than a commercial district, e.g. an outdoor mall.  the noise ratings make me think that i am thinking about this incorrectly. what is a better way of thinking of them?\\n'"},"metadata":{}}]},{"cell_type":"code","source":"def remove_words_with_nums(s):\n  return re.sub(r\"\\S*\\d\\S*\", \"\", s)\n\n\ntrain_df[columns_to_clean] = train_df[columns_to_clean].apply(lambda x: x.apply(remove_words_with_nums))\ntest_df[columns_to_clean] = test_df[columns_to_clean].apply(lambda x: x.apply(remove_words_with_nums))","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:34:25.606065Z","iopub.execute_input":"2024-07-02T09:34:25.606954Z","iopub.status.idle":"2024-07-02T09:34:28.191289Z","shell.execute_reply.started":"2024-07-02T09:34:25.606923Z","shell.execute_reply":"2024-07-02T09:34:28.190358Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"def remove_special_character(s):\n  return re.sub('[^A-Za-z0-9]+', ' ', s)\ntrain_df[columns_to_clean] = train_df[columns_to_clean].apply(lambda x: x.apply(remove_special_character))\ntest_df[columns_to_clean] = test_df[columns_to_clean].apply(lambda x: x.apply(remove_special_character))","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:34:28.193201Z","iopub.execute_input":"2024-07-02T09:34:28.193479Z","iopub.status.idle":"2024-07-02T09:34:29.130325Z","shell.execute_reply.started":"2024-07-02T09:34:28.193453Z","shell.execute_reply":"2024-07-02T09:34:29.129181Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren'])\ndef remove_stopword(s):\n    res = ' '.join([word for word in s.split(' ') if word not in stopwords])\n    return res\ntrain_df[columns_to_clean] = train_df[columns_to_clean].apply(lambda x: x.apply(remove_stopword))\ntest_df[columns_to_clean] = test_df[columns_to_clean].apply(lambda x: x.apply(remove_stopword))","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:34:29.131725Z","iopub.execute_input":"2024-07-02T09:34:29.132101Z","iopub.status.idle":"2024-07-02T09:34:29.532167Z","shell.execute_reply.started":"2024-07-02T09:34:29.132068Z","shell.execute_reply":"2024-07-02T09:34:29.531145Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('wordnet')\nnltk.download('omw-1.4')  # This is often needed for WordNet Lemmatizer","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:34:32.257533Z","iopub.execute_input":"2024-07-02T09:34:32.258494Z","iopub.status.idle":"2024-07-02T09:34:32.265138Z","shell.execute_reply.started":"2024-07-02T09:34:32.258446Z","shell.execute_reply":"2024-07-02T09:34:32.264243Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"},{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# import nltk\n# from nltk.stem import WordNetLemmatizer\n\n# # Download the necessary NLTK data\n# nltk.download('wordnet')\n# nltk.download('omw-1.4')\n\n# # Initialize the lemmatizer\n# lemmatizer = WordNetLemmatizer()\n\n# def lemmatization(s):\n#     res = ' '.join([lemmatizer.lemmatize(word) for word in s.split(' ')])\n#     return res\n\n# columns_to_clean = ['question_body', 'question_title', 'answer']\n# train_df[columns_to_clean] = train_df[columns_to_clean].apply(lambda x: x.apply(lemmatization))\n# test_df[columns_to_clean] = test_df[columns_to_clean].apply(lambda x: x.apply(lemmatization))","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:34:33.220818Z","iopub.execute_input":"2024-07-02T09:34:33.221813Z","iopub.status.idle":"2024-07-02T09:34:33.226392Z","shell.execute_reply.started":"2024-07-02T09:34:33.221778Z","shell.execute_reply":"2024-07-02T09:34:33.225301Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"# import os\n# import pandas as pd\n# import torch\n# from torch import nn\n# from transformers import BertTokenizer, BertModel, BertConfig\n\n# # Set the local paths\n# local_model_path = '/kaggle/input/bert/pytorch/bert/1'\n\n# # Load the tokenizer and model from the local files\n# tokenizer = BertTokenizer.from_pretrained(local_model_path)\n# config = BertConfig.from_pretrained(local_model_path)\n# bert_model = BertModel.from_pretrained(local_model_path, config=config, ignore_mismatched_sizes=True)\n\n# def preprocess_data(df):\n#     inputs1 = tokenizer(\n#         df['question_title'].tolist(), \n#         df['question_body'].tolist(), \n#         return_tensors='pt', \n#         padding=True, \n#         truncation=True, \n#         max_length=512\n#     )\n#     inputs2 = tokenizer(\n#         df['question_title'].tolist(), \n#         df['answer'].tolist(), \n#         return_tensors='pt', \n#         padding=True, \n#         truncation=True, \n#         max_length=512\n#     )\n#     inputs3 = tokenizer(\n#         df['question_body'].tolist(), \n#         df['answer'].tolist(), \n#         return_tensors='pt', \n#         padding=True, \n#         truncation=True, \n#         max_length=512\n#     )\n    \n#     return inputs1, inputs2, inputs3\n\n# # Preprocess data\n# inputs1, inputs2, inputs3 = preprocess_data(train_df)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:34:34.184677Z","iopub.execute_input":"2024-07-02T09:34:34.185046Z","iopub.status.idle":"2024-07-02T09:34:34.190659Z","shell.execute_reply.started":"2024-07-02T09:34:34.185015Z","shell.execute_reply":"2024-07-02T09:34:34.189565Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nfrom torch import nn\n# from transformers import DistilBertTokenizer, DistilBertModel, DistilBertConfig\n\n# # Set the local paths\n# local_model_path = '/kaggle/input/distilbert/pytorch/distilbert/1'\n\nfrom transformers import DistilBertTokenizer, DistilBertModel\n\n# Load the tokenizer and model from Hugging Face\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = DistilBertModel.from_pretrained('distilbert-base-uncased')\n\n# # Load the tokenizer and model from the local files\n# tokenizer = DistilBertTokenizer.from_pretrained(local_model_path)\n# config = DistilBertConfig.from_pretrained(local_model_path)\n# distilbert_model = DistilBertModel.from_pretrained(local_model_path, config=config, ignore_mismatched_sizes=True)\n\ndef preprocess_data(df):\n    inputs1 = tokenizer(\n        df['question_title'].tolist(), \n        df['question_body'].tolist(), \n        return_tensors='pt', \n        padding=True, \n        truncation=True, \n        max_length=512\n    )\n    inputs2 = tokenizer(\n        df['question_title'].tolist(), \n        df['answer'].tolist(), \n        return_tensors='pt', \n        padding=True, \n        truncation=True, \n        max_length=512\n    )\n    inputs3 = tokenizer(\n        df['question_body'].tolist(), \n        df['answer'].tolist(), \n        return_tensors='pt', \n        padding=True, \n        truncation=True, \n        max_length=512\n    )\n    \n    return inputs1, inputs2, inputs3\n\n# Preprocess data\ninputs1, inputs2, inputs3 = preprocess_data(train_df)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:34:34.783206Z","iopub.execute_input":"2024-07-02T09:34:34.783956Z","iopub.status.idle":"2024-07-02T09:36:07.352908Z","shell.execute_reply.started":"2024-07-02T09:34:34.783924Z","shell.execute_reply":"2024-07-02T09:36:07.352074Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"}]},{"cell_type":"code","source":"target_cols = [\n    'question_asker_intent_understanding', 'question_body_critical', 'question_conversational',\n    'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer',\n    'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent',\n    'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice',\n    'question_type_compare', 'question_type_consequence', 'question_type_definition',\n    'question_type_entity', 'question_type_instructions', 'question_type_procedure',\n    'question_type_reason_explanation', 'question_type_spelling', 'question_well_written',\n    'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n    'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure',\n    'answer_type_reason_explanation', 'answer_well_written'\n]","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:36:07.354808Z","iopub.execute_input":"2024-07-02T09:36:07.355098Z","iopub.status.idle":"2024-07-02T09:36:07.360796Z","shell.execute_reply.started":"2024-07-02T09:36:07.355071Z","shell.execute_reply":"2024-07-02T09:36:07.359751Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"# class MultiBERTModel(nn.Module):\n#     def __init__(self):\n#         super(MultiBERTModel, self).__init__()\n#         self.bert1 = BertModel.from_pretrained(local_model_path, ignore_mismatched_sizes=True)\n#         self.bert2 = BertModel.from_pretrained(local_model_path, ignore_mismatched_sizes=True)\n#         self.bert3 = BertModel.from_pretrained(local_model_path, ignore_mismatched_sizes=True)\n#         self.dropout = nn.Dropout(0.3)\n#         self.linear = nn.Linear(3 * self.bert1.config.hidden_size, len(target_cols))\n\n#     def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2, input_ids3, attention_mask3):\n#         outputs1 = self.bert1(input_ids1, attention_mask=attention_mask1)\n#         outputs2 = self.bert2(input_ids2, attention_mask=attention_mask2)\n#         outputs3 = self.bert3(input_ids3, attention_mask=attention_mask3)\n        \n#         cls_output1 = outputs1.pooler_output\n#         cls_output2 = outputs2.pooler_output\n#         cls_output3 = outputs3.pooler_output\n        \n#         concatenated_output = torch.cat((cls_output1, cls_output2, cls_output3), dim=1)\n#         dropout_output = self.dropout(concatenated_output)\n#         final_output = self.linear(dropout_output)\n        \n#         return final_output","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:36:07.361794Z","iopub.execute_input":"2024-07-02T09:36:07.362050Z","iopub.status.idle":"2024-07-02T09:36:07.380058Z","shell.execute_reply.started":"2024-07-02T09:36:07.362027Z","shell.execute_reply":"2024-07-02T09:36:07.379172Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import spearmanr\nfrom torch.cuda.amp import GradScaler, autocast","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:36:07.382615Z","iopub.execute_input":"2024-07-02T09:36:07.383310Z","iopub.status.idle":"2024-07-02T09:36:07.394114Z","shell.execute_reply.started":"2024-07-02T09:36:07.383275Z","shell.execute_reply":"2024-07-02T09:36:07.393259Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"class MultiDistilBERTModel(nn.Module):\n    def __init__(self):\n        super(MultiDistilBERTModel, self).__init__()\n        self.distilbert1 = DistilBertModel.from_pretrained('distilbert-base-uncased')\n        self.distilbert2 = DistilBertModel.from_pretrained('distilbert-base-uncased')\n        self.distilbert3 = DistilBertModel.from_pretrained('distilbert-base-uncased')\n        self.dropout = nn.Dropout(0.3)\n        self.linear = nn.Linear(3 * self.distilbert1.config.hidden_size, len(target_cols))\n\n    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2, input_ids3, attention_mask3):\n        outputs1 = self.distilbert1(input_ids1, attention_mask=attention_mask1)\n        outputs2 = self.distilbert2(input_ids2, attention_mask=attention_mask2)\n        outputs3 = self.distilbert3(input_ids3, attention_mask=attention_mask3)\n        \n        cls_output1 = outputs1.last_hidden_state[:, 0]  # CLS token\n        cls_output2 = outputs2.last_hidden_state[:, 0]  # CLS token\n        cls_output3 = outputs3.last_hidden_state[:, 0]  # CLS token\n        \n        concatenated_output = torch.cat((cls_output1, cls_output2, cls_output3), dim=1)\n        dropout_output = self.dropout(concatenated_output)\n        final_output = self.linear(dropout_output)\n        \n        return final_output","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:36:07.395348Z","iopub.execute_input":"2024-07-02T09:36:07.395694Z","iopub.status.idle":"2024-07-02T09:36:07.405801Z","shell.execute_reply.started":"2024-07-02T09:36:07.395671Z","shell.execute_reply":"2024-07-02T09:36:07.405022Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"def mean_column_wise_spearman(preds, labels):\n    preds = preds.cpu().detach().numpy()\n    labels = labels.cpu().detach().numpy()\n    num_cols = preds.shape[1]\n    spearman_coeffs = []\n    for col in range(num_cols):\n        if np.std(preds[:, col]) == 0 or np.std(labels[:, col]) == 0:\n            continue  # Ignore constant columns\n        spearman_coeff, _ = spearmanr(preds[:, col], labels[:, col])\n        spearman_coeffs.append(spearman_coeff)\n    mean_spearman = np.nanmean(spearman_coeffs) if spearman_coeffs else np.nan\n    return mean_spearman","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:36:07.406947Z","iopub.execute_input":"2024-07-02T09:36:07.407382Z","iopub.status.idle":"2024-07-02T09:36:07.421495Z","shell.execute_reply.started":"2024-07-02T09:36:07.407349Z","shell.execute_reply":"2024-07-02T09:36:07.420599Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"# # Modify the batch size to a smaller value\n# batch_size = 6\n\n# def train_model(model, train_df, inputs1, inputs2, inputs3, epochs=1, batch_size=4):\n#     model.train()\n#     optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n#     criterion = nn.BCEWithLogitsLoss()\n\n#     for epoch in range(epochs):\n#         total_loss = 0\n#         for i in range(0, len(train_df), batch_size):\n#             batch_inputs1 = {key: val[i:i+batch_size] for key, val in inputs1.items()}\n#             batch_inputs2 = {key: val[i:i+batch_size] for key, val in inputs2.items()}\n#             batch_inputs3 = {key: val[i:i+batch_size] for key, val in inputs3.items()}\n#             labels = torch.tensor(train_df.iloc[i:i+batch_size][target_cols].values, dtype=torch.float32)\n\n#             optimizer.zero_grad()\n#             outputs = model(\n#                 batch_inputs1['input_ids'], batch_inputs1['attention_mask'],\n#                 batch_inputs2['input_ids'], batch_inputs2['attention_mask'],\n#                 batch_inputs3['input_ids'], batch_inputs3['attention_mask']\n#             )\n#             loss = criterion(outputs, labels)\n#             loss.backward()\n#             optimizer.step()\n#             total_loss += loss.item()\n\n#         print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss}\")\n\n# # Initialize model\n# model = MultiBERTModel()\n# # Train the model with reduced batch size\n# train_model(model, train_df, inputs1, inputs2, inputs3, batch_size=batch_size)\n# print('done')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:36:07.422685Z","iopub.execute_input":"2024-07-02T09:36:07.422956Z","iopub.status.idle":"2024-07-02T09:36:07.432777Z","shell.execute_reply.started":"2024-07-02T09:36:07.422934Z","shell.execute_reply":"2024-07-02T09:36:07.431889Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"def train_model(model, train_df, inputs1, inputs2, inputs3, epochs=3, batch_size=6):\n    model.train()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n    criterion = nn.BCEWithLogitsLoss()\n    scaler = GradScaler()  # Mixed precision training\n\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n        model = model.to(device)\n        print(\"Training on GPU\")\n    else:\n        device = torch.device(\"cpu\")\n        print(\"Training on CPU\")\n    \n    for epoch in range(epochs):\n        total_loss = 0\n        spearman_scores = []\n        for i in range(0, len(train_df), batch_size):\n            batch_inputs1 = {key: val[i:i+batch_size].to(device) for key, val in inputs1.items()}\n            batch_inputs2 = {key: val[i:i+batch_size].to(device) for key, val in inputs2.items()}\n            batch_inputs3 = {key: val[i:i+batch_size].to(device) for key, val in inputs3.items()}\n            labels = torch.tensor(train_df.iloc[i:i+batch_size][target_cols].values, dtype=torch.float32).to(device)\n            \n            optimizer.zero_grad()\n            \n            with autocast():  # Mixed precision training\n                outputs = model(\n                    batch_inputs1['input_ids'], batch_inputs1['attention_mask'],\n                    batch_inputs2['input_ids'], batch_inputs2['attention_mask'],\n                    batch_inputs3['input_ids'], batch_inputs3['attention_mask']\n                )\n                loss = criterion(outputs, labels)\n            \n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            \n            total_loss += loss.item()\n\n            # Calculate and accumulate mean Spearman's correlation coefficient\n            mean_spearman = mean_column_wise_spearman(outputs, labels)\n            if not np.isnan(mean_spearman):\n                spearman_scores.append(mean_spearman)\n            \n            # Print training progress for the current batch\n            if (i // batch_size + 1) % 10 == 0:\n                print(f\"Batch {i // batch_size + 1}/{len(train_df) // batch_size}, Loss: {total_loss / (i // batch_size + 1)}, Mean Spearman (current batch): {mean_spearman:.4f}\")\n\n            # Clear cache\n            torch.cuda.empty_cache()\n        \n        # Print epoch results\n        overall_mean_spearman = np.nanmean(spearman_scores)\n        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss}, Overall Mean Spearman: {overall_mean_spearman:.4f}\")\n\n# Initialize model\nmodel = MultiDistilBERTModel()\n# Train the model with reduced batch size\ntrain_model(model, train_df, inputs1, inputs2, inputs3, batch_size=batch_size)\nprint('done')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:36:07.434211Z","iopub.execute_input":"2024-07-02T09:36:07.434741Z","iopub.status.idle":"2024-07-02T10:06:34.044658Z","shell.execute_reply.started":"2024-07-02T09:36:07.434706Z","shell.execute_reply":"2024-07-02T10:06:34.043557Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"Training on GPU\nBatch 10/1519, Loss: 0.6524853110313416, Mean Spearman (current batch): -0.3177\nBatch 20/1519, Loss: 0.5948165237903595, Mean Spearman (current batch): 0.1173\nBatch 30/1519, Loss: 0.552901703119278, Mean Spearman (current batch): 0.2311\nBatch 40/1519, Loss: 0.5235749028623105, Mean Spearman (current batch): -0.0951\nBatch 50/1519, Loss: 0.5075132793188095, Mean Spearman (current batch): 0.0496\nBatch 60/1519, Loss: 0.4981283684571584, Mean Spearman (current batch): 0.1265\nBatch 70/1519, Loss: 0.4878152536494391, Mean Spearman (current batch): 0.0947\nBatch 80/1519, Loss: 0.4809901598840952, Mean Spearman (current batch): 0.1064\nBatch 90/1519, Loss: 0.4765071170197593, Mean Spearman (current batch): 0.1122\nBatch 100/1519, Loss: 0.4723880076408386, Mean Spearman (current batch): -0.1086\nBatch 110/1519, Loss: 0.46892104663632134, Mean Spearman (current batch): 0.0986\nBatch 120/1519, Loss: 0.4648064238329728, Mean Spearman (current batch): 0.0494\nBatch 130/1519, Loss: 0.46239968354885397, Mean Spearman (current batch): 0.2350\nBatch 140/1519, Loss: 0.458872038764613, Mean Spearman (current batch): 0.1812\nBatch 150/1519, Loss: 0.4560251541932424, Mean Spearman (current batch): -0.0011\nBatch 160/1519, Loss: 0.45357696432620287, Mean Spearman (current batch): 0.1332\nBatch 170/1519, Loss: 0.45056605286457957, Mean Spearman (current batch): 0.1658\nBatch 180/1519, Loss: 0.44786974324120415, Mean Spearman (current batch): -0.0765\nBatch 190/1519, Loss: 0.4468378456015336, Mean Spearman (current batch): 0.0539\nBatch 200/1519, Loss: 0.44568333491683004, Mean Spearman (current batch): -0.0134\nBatch 210/1519, Loss: 0.4430572004545303, Mean Spearman (current batch): 0.3528\nBatch 220/1519, Loss: 0.44218227158893236, Mean Spearman (current batch): 0.2898\nBatch 230/1519, Loss: 0.4423003919746565, Mean Spearman (current batch): 0.0517\nBatch 240/1519, Loss: 0.4410521057744821, Mean Spearman (current batch): 0.1551\nBatch 250/1519, Loss: 0.4388583769798279, Mean Spearman (current batch): 0.3758\nBatch 260/1519, Loss: 0.4381375865294383, Mean Spearman (current batch): 0.4029\nBatch 270/1519, Loss: 0.43714813071268577, Mean Spearman (current batch): 0.3030\nBatch 280/1519, Loss: 0.43629598351461546, Mean Spearman (current batch): 0.2781\nBatch 290/1519, Loss: 0.4355156926245525, Mean Spearman (current batch): 0.1966\nBatch 300/1519, Loss: 0.43443754603465395, Mean Spearman (current batch): 0.5432\nBatch 310/1519, Loss: 0.4336421889643515, Mean Spearman (current batch): 0.3539\nBatch 320/1519, Loss: 0.43275629738345744, Mean Spearman (current batch): 0.2011\nBatch 330/1519, Loss: 0.43193236751989883, Mean Spearman (current batch): 0.2921\nBatch 340/1519, Loss: 0.4314483030753977, Mean Spearman (current batch): 0.0100\nBatch 350/1519, Loss: 0.4308723535708019, Mean Spearman (current batch): 0.1700\nBatch 360/1519, Loss: 0.430019950783915, Mean Spearman (current batch): 0.1629\nBatch 370/1519, Loss: 0.4296367437452883, Mean Spearman (current batch): 0.2785\nBatch 380/1519, Loss: 0.4290747876230039, Mean Spearman (current batch): 0.3675\nBatch 390/1519, Loss: 0.42871397000092726, Mean Spearman (current batch): 0.3545\nBatch 400/1519, Loss: 0.4278778725117445, Mean Spearman (current batch): 0.1819\nBatch 410/1519, Loss: 0.42794276468637515, Mean Spearman (current batch): 0.2924\nBatch 420/1519, Loss: 0.4269614529751596, Mean Spearman (current batch): 0.1676\nBatch 430/1519, Loss: 0.4264602769252866, Mean Spearman (current batch): 0.4432\nBatch 440/1519, Loss: 0.42601912367072975, Mean Spearman (current batch): 0.2843\nBatch 450/1519, Loss: 0.42586723274654814, Mean Spearman (current batch): 0.2130\nBatch 460/1519, Loss: 0.425779341938703, Mean Spearman (current batch): 0.3084\nBatch 470/1519, Loss: 0.42538106917066776, Mean Spearman (current batch): 0.0988\nBatch 480/1519, Loss: 0.42524324245750905, Mean Spearman (current batch): 0.5759\nBatch 490/1519, Loss: 0.4251027138865724, Mean Spearman (current batch): 0.2516\nBatch 500/1519, Loss: 0.42452024656534193, Mean Spearman (current batch): 0.2341\nBatch 510/1519, Loss: 0.4240394934135325, Mean Spearman (current batch): -0.0143\nBatch 520/1519, Loss: 0.42349523580991305, Mean Spearman (current batch): 0.4179\nBatch 530/1519, Loss: 0.42266898678158815, Mean Spearman (current batch): -0.1946\nBatch 540/1519, Loss: 0.42238698132612085, Mean Spearman (current batch): 0.2151\nBatch 550/1519, Loss: 0.4220772928541357, Mean Spearman (current batch): -0.0363\nBatch 560/1519, Loss: 0.42170253465218205, Mean Spearman (current batch): 0.1190\nBatch 570/1519, Loss: 0.42141156065882296, Mean Spearman (current batch): 0.3310\nBatch 580/1519, Loss: 0.42088004438013865, Mean Spearman (current batch): 0.3016\nBatch 590/1519, Loss: 0.4207724089339628, Mean Spearman (current batch): 0.2510\nBatch 600/1519, Loss: 0.4201464611291885, Mean Spearman (current batch): 0.2459\nBatch 610/1519, Loss: 0.41985846508721836, Mean Spearman (current batch): 0.4544\nBatch 620/1519, Loss: 0.4192842137429022, Mean Spearman (current batch): 0.2870\nBatch 630/1519, Loss: 0.41888321094096653, Mean Spearman (current batch): 0.2434\nBatch 640/1519, Loss: 0.41825267910026015, Mean Spearman (current batch): 0.1761\nBatch 650/1519, Loss: 0.41826242777017447, Mean Spearman (current batch): 0.4281\nBatch 660/1519, Loss: 0.41794001166567657, Mean Spearman (current batch): 0.2769\nBatch 670/1519, Loss: 0.4178130727650514, Mean Spearman (current batch): 0.1864\nBatch 680/1519, Loss: 0.4172866084119853, Mean Spearman (current batch): 0.2857\nBatch 690/1519, Loss: 0.41706659495830534, Mean Spearman (current batch): 0.4813\nBatch 700/1519, Loss: 0.41679826872689385, Mean Spearman (current batch): 0.3474\nBatch 710/1519, Loss: 0.4169034766479277, Mean Spearman (current batch): 0.1041\nBatch 720/1519, Loss: 0.41669804553190865, Mean Spearman (current batch): 0.2283\nBatch 730/1519, Loss: 0.41620523562170053, Mean Spearman (current batch): 0.2572\nBatch 740/1519, Loss: 0.4158605017774814, Mean Spearman (current batch): 0.0906\nBatch 750/1519, Loss: 0.41575466720263166, Mean Spearman (current batch): 0.4962\nBatch 760/1519, Loss: 0.4156907370215968, Mean Spearman (current batch): 0.0232\nBatch 770/1519, Loss: 0.41521707328109, Mean Spearman (current batch): 0.0493\nBatch 780/1519, Loss: 0.4153161516556373, Mean Spearman (current batch): 0.4195\nBatch 790/1519, Loss: 0.41509477073633216, Mean Spearman (current batch): 0.4696\nBatch 800/1519, Loss: 0.415047279894352, Mean Spearman (current batch): 0.2308\nBatch 810/1519, Loss: 0.41483897103203665, Mean Spearman (current batch): 0.4422\nBatch 820/1519, Loss: 0.4148342657016545, Mean Spearman (current batch): -0.0665\nBatch 830/1519, Loss: 0.4142369853085782, Mean Spearman (current batch): 0.1934\nBatch 840/1519, Loss: 0.4142575659212612, Mean Spearman (current batch): 0.1200\nBatch 850/1519, Loss: 0.414073906120132, Mean Spearman (current batch): 0.0733\nBatch 860/1519, Loss: 0.4141170747404875, Mean Spearman (current batch): 0.1350\nBatch 870/1519, Loss: 0.4139323083491161, Mean Spearman (current batch): 0.3049\nBatch 880/1519, Loss: 0.4138874643228271, Mean Spearman (current batch): 0.2501\nBatch 890/1519, Loss: 0.4135849681128277, Mean Spearman (current batch): 0.3037\nBatch 900/1519, Loss: 0.41331369969579906, Mean Spearman (current batch): 0.4456\nBatch 910/1519, Loss: 0.41317137643531127, Mean Spearman (current batch): 0.2261\nBatch 920/1519, Loss: 0.41301345757168273, Mean Spearman (current batch): 0.1786\nBatch 930/1519, Loss: 0.41270366480914494, Mean Spearman (current batch): 0.3422\nBatch 940/1519, Loss: 0.412432251807223, Mean Spearman (current batch): 0.5278\nBatch 950/1519, Loss: 0.4121139969010102, Mean Spearman (current batch): 0.3914\nBatch 960/1519, Loss: 0.41191227979337175, Mean Spearman (current batch): 0.3620\nBatch 970/1519, Loss: 0.4116667057742778, Mean Spearman (current batch): 0.3171\nBatch 980/1519, Loss: 0.411706484033137, Mean Spearman (current batch): 0.2545\nBatch 990/1519, Loss: 0.41153332600087833, Mean Spearman (current batch): 0.2315\nBatch 1000/1519, Loss: 0.41131848078966143, Mean Spearman (current batch): 0.3315\nBatch 1010/1519, Loss: 0.4111619729216736, Mean Spearman (current batch): 0.4841\nBatch 1020/1519, Loss: 0.4108571220262378, Mean Spearman (current batch): 0.2264\nBatch 1030/1519, Loss: 0.4107006311127283, Mean Spearman (current batch): -0.0228\nBatch 1040/1519, Loss: 0.4107470888071335, Mean Spearman (current batch): 0.3376\nBatch 1050/1519, Loss: 0.41063289449328466, Mean Spearman (current batch): 0.2303\nBatch 1060/1519, Loss: 0.41064429108826617, Mean Spearman (current batch): 0.1339\nBatch 1070/1519, Loss: 0.4105718180119434, Mean Spearman (current batch): 0.2135\nBatch 1080/1519, Loss: 0.4103602049527345, Mean Spearman (current batch): 0.3749\nBatch 1090/1519, Loss: 0.41023993133951764, Mean Spearman (current batch): 0.2795\nBatch 1100/1519, Loss: 0.41031355914744466, Mean Spearman (current batch): 0.2216\nBatch 1110/1519, Loss: 0.410185285248198, Mean Spearman (current batch): 0.0529\nBatch 1120/1519, Loss: 0.4100742716874395, Mean Spearman (current batch): 0.3630\nBatch 1130/1519, Loss: 0.4097654815532465, Mean Spearman (current batch): 0.2036\nBatch 1140/1519, Loss: 0.4096320774471551, Mean Spearman (current batch): 0.3008\nBatch 1150/1519, Loss: 0.40941716932732125, Mean Spearman (current batch): 0.3857\nBatch 1160/1519, Loss: 0.40936581849538045, Mean Spearman (current batch): 0.1113\nBatch 1170/1519, Loss: 0.40932402052940464, Mean Spearman (current batch): 0.3298\nBatch 1180/1519, Loss: 0.409063134582366, Mean Spearman (current batch): 0.3465\nBatch 1190/1519, Loss: 0.40896906389408755, Mean Spearman (current batch): 0.0049\nBatch 1200/1519, Loss: 0.40876961266001066, Mean Spearman (current batch): 0.2945\nBatch 1210/1519, Loss: 0.40866395808941075, Mean Spearman (current batch): 0.1286\nBatch 1220/1519, Loss: 0.4084682628268101, Mean Spearman (current batch): 0.3789\nBatch 1230/1519, Loss: 0.40853822623811115, Mean Spearman (current batch): 0.4602\nBatch 1240/1519, Loss: 0.4084418195870615, Mean Spearman (current batch): 0.0153\nBatch 1250/1519, Loss: 0.4084098601579666, Mean Spearman (current batch): 0.4531\nBatch 1260/1519, Loss: 0.40826198427923144, Mean Spearman (current batch): 0.1753\nBatch 1270/1519, Loss: 0.4079542629362091, Mean Spearman (current batch): 0.1531\nBatch 1280/1519, Loss: 0.4079369590384886, Mean Spearman (current batch): 0.4750\nBatch 1290/1519, Loss: 0.4078951029583465, Mean Spearman (current batch): 0.4659\nBatch 1300/1519, Loss: 0.4077095415500494, Mean Spearman (current batch): 0.2614\nBatch 1310/1519, Loss: 0.4075180903190875, Mean Spearman (current batch): 0.3057\nBatch 1320/1519, Loss: 0.40742512221137683, Mean Spearman (current batch): 0.2672\nBatch 1330/1519, Loss: 0.4073181676013129, Mean Spearman (current batch): 0.0867\nBatch 1340/1519, Loss: 0.4074715357218216, Mean Spearman (current batch): 0.3111\nBatch 1350/1519, Loss: 0.4074124957897045, Mean Spearman (current batch): 0.3168\nBatch 1360/1519, Loss: 0.407288245812935, Mean Spearman (current batch): 0.0032\nBatch 1370/1519, Loss: 0.407041829171842, Mean Spearman (current batch): 0.5368\nBatch 1380/1519, Loss: 0.40694271049637726, Mean Spearman (current batch): 0.3813\nBatch 1390/1519, Loss: 0.40674853204823225, Mean Spearman (current batch): 0.3413\nBatch 1400/1519, Loss: 0.4066425969132355, Mean Spearman (current batch): 0.3027\nBatch 1410/1519, Loss: 0.4064383418847483, Mean Spearman (current batch): 0.2567\nBatch 1420/1519, Loss: 0.4062882571153238, Mean Spearman (current batch): 0.3599\nBatch 1430/1519, Loss: 0.4063531467130968, Mean Spearman (current batch): 0.0980\nBatch 1440/1519, Loss: 0.40609637863106196, Mean Spearman (current batch): 0.2673\nBatch 1450/1519, Loss: 0.4061045097071549, Mean Spearman (current batch): 0.2811\nBatch 1460/1519, Loss: 0.40607512699414605, Mean Spearman (current batch): 0.4027\nBatch 1470/1519, Loss: 0.4059730171954551, Mean Spearman (current batch): 0.3471\nBatch 1480/1519, Loss: 0.4059616129341963, Mean Spearman (current batch): 0.4978\nBatch 1490/1519, Loss: 0.4058572215721911, Mean Spearman (current batch): 0.4187\nBatch 1500/1519, Loss: 0.4058169877529144, Mean Spearman (current batch): 0.0636\nBatch 1510/1519, Loss: 0.40580280016589637, Mean Spearman (current batch): 0.0031\nBatch 1520/1519, Loss: 0.4056882404967358, Mean Spearman (current batch): 0.5186\nEpoch 1/3, Loss: 616.6461255550385, Overall Mean Spearman: 0.2376\nBatch 10/1519, Loss: 0.40381133258342744, Mean Spearman (current batch): 0.3501\nBatch 20/1519, Loss: 0.38915602713823316, Mean Spearman (current batch): 0.3385\nBatch 30/1519, Loss: 0.3865705152352651, Mean Spearman (current batch): 0.3367\nBatch 40/1519, Loss: 0.38567655608057977, Mean Spearman (current batch): 0.1889\nBatch 50/1519, Loss: 0.38820698618888855, Mean Spearman (current batch): 0.3493\nBatch 60/1519, Loss: 0.3900038282076518, Mean Spearman (current batch): 0.5158\nBatch 70/1519, Loss: 0.38952166140079497, Mean Spearman (current batch): 0.3412\nBatch 80/1519, Loss: 0.39123048894107343, Mean Spearman (current batch): 0.4592\nBatch 90/1519, Loss: 0.39243090550104776, Mean Spearman (current batch): 0.4339\nBatch 100/1519, Loss: 0.3941020232439041, Mean Spearman (current batch): 0.1463\nBatch 110/1519, Loss: 0.39556010663509367, Mean Spearman (current batch): 0.4483\nBatch 120/1519, Loss: 0.3951178893446922, Mean Spearman (current batch): 0.2806\nBatch 130/1519, Loss: 0.39637178503550013, Mean Spearman (current batch): 0.1601\nBatch 140/1519, Loss: 0.39550076467650275, Mean Spearman (current batch): 0.1536\nBatch 150/1519, Loss: 0.39447555840015414, Mean Spearman (current batch): 0.3791\nBatch 160/1519, Loss: 0.39349359162151815, Mean Spearman (current batch): 0.1508\nBatch 170/1519, Loss: 0.3921468643581166, Mean Spearman (current batch): 0.5270\nBatch 180/1519, Loss: 0.39203761567672096, Mean Spearman (current batch): 0.2344\nBatch 190/1519, Loss: 0.3921805511963995, Mean Spearman (current batch): 0.3520\nBatch 200/1519, Loss: 0.39269363224506376, Mean Spearman (current batch): 0.1162\nBatch 210/1519, Loss: 0.3914970950001762, Mean Spearman (current batch): 0.4363\nBatch 220/1519, Loss: 0.39148045615716415, Mean Spearman (current batch): 0.2956\nBatch 230/1519, Loss: 0.39275276375853496, Mean Spearman (current batch): 0.0068\nBatch 240/1519, Loss: 0.3924257864554723, Mean Spearman (current batch): 0.1726\nBatch 250/1519, Loss: 0.39153788292407987, Mean Spearman (current batch): 0.4085\nBatch 260/1519, Loss: 0.3917087020782324, Mean Spearman (current batch): 0.6216\nBatch 270/1519, Loss: 0.39158512905791953, Mean Spearman (current batch): 0.4125\nBatch 280/1519, Loss: 0.39132227269666536, Mean Spearman (current batch): 0.4913\nBatch 290/1519, Loss: 0.3912374524206951, Mean Spearman (current batch): 0.3053\nBatch 300/1519, Loss: 0.39084589382012686, Mean Spearman (current batch): 0.5289\nBatch 310/1519, Loss: 0.39087423672599175, Mean Spearman (current batch): 0.2844\nBatch 320/1519, Loss: 0.39049791507422926, Mean Spearman (current batch): 0.4296\nBatch 330/1519, Loss: 0.39025062530329735, Mean Spearman (current batch): 0.4566\nBatch 340/1519, Loss: 0.39010355910834144, Mean Spearman (current batch): 0.2512\nBatch 350/1519, Loss: 0.3900733757019043, Mean Spearman (current batch): 0.2456\nBatch 360/1519, Loss: 0.3897386169268025, Mean Spearman (current batch): 0.3439\nBatch 370/1519, Loss: 0.38983145968334093, Mean Spearman (current batch): 0.0789\nBatch 380/1519, Loss: 0.38980399252552733, Mean Spearman (current batch): 0.3616\nBatch 390/1519, Loss: 0.38984622328709334, Mean Spearman (current batch): 0.3992\nBatch 400/1519, Loss: 0.3894513101130724, Mean Spearman (current batch): 0.4766\nBatch 410/1519, Loss: 0.38971117102518316, Mean Spearman (current batch): 0.5128\nBatch 420/1519, Loss: 0.38900515607425146, Mean Spearman (current batch): 0.2236\nBatch 430/1519, Loss: 0.38884811207305553, Mean Spearman (current batch): 0.3458\nBatch 440/1519, Loss: 0.38874576173045416, Mean Spearman (current batch): 0.2956\nBatch 450/1519, Loss: 0.388797322511673, Mean Spearman (current batch): 0.3484\nBatch 460/1519, Loss: 0.38908257406690844, Mean Spearman (current batch): 0.3975\nBatch 470/1519, Loss: 0.3889298307768842, Mean Spearman (current batch): 0.1901\nBatch 480/1519, Loss: 0.38904497170199953, Mean Spearman (current batch): 0.4240\nBatch 490/1519, Loss: 0.38911347383139083, Mean Spearman (current batch): 0.3712\nBatch 500/1519, Loss: 0.38890261006355287, Mean Spearman (current batch): 0.3938\nBatch 510/1519, Loss: 0.3887023562309789, Mean Spearman (current batch): 0.0050\nBatch 520/1519, Loss: 0.38840847932375394, Mean Spearman (current batch): 0.2539\nBatch 530/1519, Loss: 0.38787596152638487, Mean Spearman (current batch): 0.2118\nBatch 540/1519, Loss: 0.3878763151941476, Mean Spearman (current batch): 0.1208\nBatch 550/1519, Loss: 0.38773409139026294, Mean Spearman (current batch): 0.1698\nBatch 560/1519, Loss: 0.3875249570501702, Mean Spearman (current batch): 0.1338\nBatch 570/1519, Loss: 0.3875029127848776, Mean Spearman (current batch): 0.3759\nBatch 580/1519, Loss: 0.38727560089579943, Mean Spearman (current batch): 0.4763\nBatch 590/1519, Loss: 0.38743997044482476, Mean Spearman (current batch): 0.3106\nBatch 600/1519, Loss: 0.386975162923336, Mean Spearman (current batch): 0.3249\nBatch 610/1519, Loss: 0.3867995894834643, Mean Spearman (current batch): 0.4881\nBatch 620/1519, Loss: 0.386466244824471, Mean Spearman (current batch): 0.4299\nBatch 630/1519, Loss: 0.38638409976921384, Mean Spearman (current batch): 0.3943\nBatch 640/1519, Loss: 0.38585247192531824, Mean Spearman (current batch): 0.1297\nBatch 650/1519, Loss: 0.38589862369574035, Mean Spearman (current batch): 0.4822\nBatch 660/1519, Loss: 0.38583161695436996, Mean Spearman (current batch): 0.2169\nBatch 670/1519, Loss: 0.38595901652058556, Mean Spearman (current batch): 0.2463\nBatch 680/1519, Loss: 0.38562826882390416, Mean Spearman (current batch): 0.4710\nBatch 690/1519, Loss: 0.3855785406154135, Mean Spearman (current batch): 0.5383\nBatch 700/1519, Loss: 0.38551752690758023, Mean Spearman (current batch): 0.4563\nBatch 710/1519, Loss: 0.3857412031838592, Mean Spearman (current batch): 0.0900\nBatch 720/1519, Loss: 0.3856678083125088, Mean Spearman (current batch): 0.4218\nBatch 730/1519, Loss: 0.3852979397528792, Mean Spearman (current batch): 0.3800\nBatch 740/1519, Loss: 0.3850229169871356, Mean Spearman (current batch): 0.2285\nBatch 750/1519, Loss: 0.38514999151229856, Mean Spearman (current batch): 0.5498\nBatch 760/1519, Loss: 0.3852992881285517, Mean Spearman (current batch): 0.1011\nBatch 770/1519, Loss: 0.3850393761288036, Mean Spearman (current batch): 0.0888\nBatch 780/1519, Loss: 0.3852634234688221, Mean Spearman (current batch): 0.5184\nBatch 790/1519, Loss: 0.3851757411715351, Mean Spearman (current batch): 0.5738\nBatch 800/1519, Loss: 0.3852403619512916, Mean Spearman (current batch): 0.4182\nBatch 810/1519, Loss: 0.3852268628500126, Mean Spearman (current batch): 0.5676\nBatch 820/1519, Loss: 0.38537575202744184, Mean Spearman (current batch): 0.1644\nBatch 830/1519, Loss: 0.3849147898605071, Mean Spearman (current batch): 0.3138\nBatch 840/1519, Loss: 0.38510242111626125, Mean Spearman (current batch): 0.0527\nBatch 850/1519, Loss: 0.3850561198416878, Mean Spearman (current batch): 0.1820\nBatch 860/1519, Loss: 0.3852132106589716, Mean Spearman (current batch): 0.2545\nBatch 870/1519, Loss: 0.385188324355531, Mean Spearman (current batch): 0.4740\nBatch 880/1519, Loss: 0.3851219064471397, Mean Spearman (current batch): 0.4410\nBatch 890/1519, Loss: 0.38494010698259545, Mean Spearman (current batch): 0.5081\nBatch 900/1519, Loss: 0.3847826180855433, Mean Spearman (current batch): 0.3523\nBatch 910/1519, Loss: 0.3847410839665067, Mean Spearman (current batch): 0.4442\nBatch 920/1519, Loss: 0.3847323676490265, Mean Spearman (current batch): 0.3111\nBatch 930/1519, Loss: 0.38459779905375613, Mean Spearman (current batch): 0.3928\nBatch 940/1519, Loss: 0.3844530614132577, Mean Spearman (current batch): 0.5111\nBatch 950/1519, Loss: 0.38424438379312814, Mean Spearman (current batch): 0.4218\nBatch 960/1519, Loss: 0.38410029743487634, Mean Spearman (current batch): 0.4554\nBatch 970/1519, Loss: 0.38400553249821223, Mean Spearman (current batch): 0.1962\nBatch 980/1519, Loss: 0.38411276900038427, Mean Spearman (current batch): 0.3376\nBatch 990/1519, Loss: 0.38400849588591646, Mean Spearman (current batch): 0.3471\nBatch 1000/1519, Loss: 0.38392898455262187, Mean Spearman (current batch): 0.2530\nBatch 1010/1519, Loss: 0.3837895847193085, Mean Spearman (current batch): 0.4522\nBatch 1020/1519, Loss: 0.38362376201970905, Mean Spearman (current batch): 0.2824\nBatch 1030/1519, Loss: 0.38360392347122857, Mean Spearman (current batch): -0.0637\nBatch 1040/1519, Loss: 0.38369338065385816, Mean Spearman (current batch): 0.2504\nBatch 1050/1519, Loss: 0.38361669790177116, Mean Spearman (current batch): 0.2094\nBatch 1060/1519, Loss: 0.3836890163005523, Mean Spearman (current batch): 0.2863\nBatch 1070/1519, Loss: 0.383739349329583, Mean Spearman (current batch): 0.3376\nBatch 1080/1519, Loss: 0.38362418998170783, Mean Spearman (current batch): 0.3079\nBatch 1090/1519, Loss: 0.3836229077719767, Mean Spearman (current batch): 0.4829\nBatch 1100/1519, Loss: 0.38375406089154157, Mean Spearman (current batch): 0.4360\nBatch 1110/1519, Loss: 0.383695632031372, Mean Spearman (current batch): 0.1304\nBatch 1120/1519, Loss: 0.3835937916434237, Mean Spearman (current batch): 0.2682\nBatch 1130/1519, Loss: 0.38341643419940913, Mean Spearman (current batch): 0.3472\nBatch 1140/1519, Loss: 0.3833444735460114, Mean Spearman (current batch): 0.3039\nBatch 1150/1519, Loss: 0.38313947768315026, Mean Spearman (current batch): 0.3245\nBatch 1160/1519, Loss: 0.38313207228122087, Mean Spearman (current batch): 0.2482\nBatch 1170/1519, Loss: 0.383148772135759, Mean Spearman (current batch): 0.4128\nBatch 1180/1519, Loss: 0.382978130018307, Mean Spearman (current batch): 0.3717\nBatch 1190/1519, Loss: 0.3829794073806089, Mean Spearman (current batch): 0.0444\nBatch 1200/1519, Loss: 0.38280959603687126, Mean Spearman (current batch): 0.3894\nBatch 1210/1519, Loss: 0.382741598058338, Mean Spearman (current batch): 0.3403\nBatch 1220/1519, Loss: 0.38260574934423947, Mean Spearman (current batch): 0.3425\nBatch 1230/1519, Loss: 0.382751766937535, Mean Spearman (current batch): 0.4764\nBatch 1240/1519, Loss: 0.3827553868534104, Mean Spearman (current batch): 0.3412\nBatch 1250/1519, Loss: 0.38278070800304415, Mean Spearman (current batch): 0.3766\nBatch 1260/1519, Loss: 0.382685469516686, Mean Spearman (current batch): 0.1844\nBatch 1270/1519, Loss: 0.3824858561510176, Mean Spearman (current batch): 0.3601\nBatch 1280/1519, Loss: 0.38253583260811863, Mean Spearman (current batch): 0.4700\nBatch 1290/1519, Loss: 0.38253052613531896, Mean Spearman (current batch): 0.5473\nBatch 1300/1519, Loss: 0.3824124015982334, Mean Spearman (current batch): 0.4354\nBatch 1310/1519, Loss: 0.38229097046014915, Mean Spearman (current batch): 0.2744\nBatch 1320/1519, Loss: 0.3822473971003836, Mean Spearman (current batch): 0.5517\nBatch 1330/1519, Loss: 0.3821588771235674, Mean Spearman (current batch): 0.3695\nBatch 1340/1519, Loss: 0.3823343471582256, Mean Spearman (current batch): 0.4105\nBatch 1350/1519, Loss: 0.3823292602433099, Mean Spearman (current batch): 0.3770\nBatch 1360/1519, Loss: 0.3822304539163323, Mean Spearman (current batch): 0.3654\nBatch 1370/1519, Loss: 0.3820135203373693, Mean Spearman (current batch): 0.6070\nBatch 1380/1519, Loss: 0.38199039468730706, Mean Spearman (current batch): 0.3238\nBatch 1390/1519, Loss: 0.38188126245848564, Mean Spearman (current batch): 0.2242\nBatch 1400/1519, Loss: 0.38182941119585717, Mean Spearman (current batch): 0.3389\nBatch 1410/1519, Loss: 0.3817198513670171, Mean Spearman (current batch): 0.1510\nBatch 1420/1519, Loss: 0.38162356098772776, Mean Spearman (current batch): 0.2864\nBatch 1430/1519, Loss: 0.3818010771608019, Mean Spearman (current batch): 0.1785\nBatch 1440/1519, Loss: 0.38159765760517783, Mean Spearman (current batch): 0.2580\nBatch 1450/1519, Loss: 0.38169823023779637, Mean Spearman (current batch): 0.4996\nBatch 1460/1519, Loss: 0.38173739014014807, Mean Spearman (current batch): 0.3480\nBatch 1470/1519, Loss: 0.38166448934143093, Mean Spearman (current batch): 0.5168\nBatch 1480/1519, Loss: 0.3816874955755633, Mean Spearman (current batch): 0.2704\nBatch 1490/1519, Loss: 0.3816485748395024, Mean Spearman (current batch): 0.4952\nBatch 1500/1519, Loss: 0.3816758769750595, Mean Spearman (current batch): 0.2260\nBatch 1510/1519, Loss: 0.3816314515491195, Mean Spearman (current batch): 0.2654\nBatch 1520/1519, Loss: 0.38156178699512233, Mean Spearman (current batch): 0.6056\nEpoch 2/3, Loss: 579.9739162325859, Overall Mean Spearman: 0.3373\nBatch 10/1519, Loss: 0.3867905616760254, Mean Spearman (current batch): 0.2421\nBatch 20/1519, Loss: 0.37322355806827545, Mean Spearman (current batch): 0.2447\nBatch 30/1519, Loss: 0.370426536599795, Mean Spearman (current batch): 0.3974\nBatch 40/1519, Loss: 0.36883766278624536, Mean Spearman (current batch): 0.3163\nBatch 50/1519, Loss: 0.37030540108680726, Mean Spearman (current batch): 0.2828\nBatch 60/1519, Loss: 0.37138482332229616, Mean Spearman (current batch): 0.5920\nBatch 70/1519, Loss: 0.37124575929982323, Mean Spearman (current batch): 0.4351\nBatch 80/1519, Loss: 0.37322889529168607, Mean Spearman (current batch): 0.4923\nBatch 90/1519, Loss: 0.3746437887350718, Mean Spearman (current batch): 0.4006\nBatch 100/1519, Loss: 0.37580029517412183, Mean Spearman (current batch): 0.3273\nBatch 110/1519, Loss: 0.3768004823814739, Mean Spearman (current batch): 0.5530\nBatch 120/1519, Loss: 0.37660693302750586, Mean Spearman (current batch): 0.4825\nBatch 130/1519, Loss: 0.3783479385651075, Mean Spearman (current batch): 0.2816\nBatch 140/1519, Loss: 0.37831614932843616, Mean Spearman (current batch): 0.3275\nBatch 150/1519, Loss: 0.37734262029329935, Mean Spearman (current batch): 0.4238\nBatch 160/1519, Loss: 0.3763353398069739, Mean Spearman (current batch): 0.0573\nBatch 170/1519, Loss: 0.37520381405073056, Mean Spearman (current batch): 0.5622\nBatch 180/1519, Loss: 0.3753770669301351, Mean Spearman (current batch): 0.0725\nBatch 190/1519, Loss: 0.3758907322820864, Mean Spearman (current batch): 0.6575\nBatch 200/1519, Loss: 0.3764157612621784, Mean Spearman (current batch): 0.3480\nBatch 210/1519, Loss: 0.3755355132477624, Mean Spearman (current batch): 0.3750\nBatch 220/1519, Loss: 0.3754265235228972, Mean Spearman (current batch): 0.3609\nBatch 230/1519, Loss: 0.37681668180486433, Mean Spearman (current batch): 0.1859\nBatch 240/1519, Loss: 0.37666884424785774, Mean Spearman (current batch): 0.1795\nBatch 250/1519, Loss: 0.3758204289674759, Mean Spearman (current batch): 0.3919\nBatch 260/1519, Loss: 0.37595303379572353, Mean Spearman (current batch): 0.6246\nBatch 270/1519, Loss: 0.37579475586061123, Mean Spearman (current batch): 0.3949\nBatch 280/1519, Loss: 0.37541658207774165, Mean Spearman (current batch): 0.5238\nBatch 290/1519, Loss: 0.375478281440406, Mean Spearman (current batch): 0.1048\nBatch 300/1519, Loss: 0.3751039073864619, Mean Spearman (current batch): 0.6224\nBatch 310/1519, Loss: 0.37491491311980835, Mean Spearman (current batch): 0.3387\nBatch 320/1519, Loss: 0.37438690373674033, Mean Spearman (current batch): 0.4288\nBatch 330/1519, Loss: 0.3743773510058721, Mean Spearman (current batch): 0.4368\nBatch 340/1519, Loss: 0.37412825857891757, Mean Spearman (current batch): 0.2258\nBatch 350/1519, Loss: 0.37408628284931184, Mean Spearman (current batch): 0.3693\nBatch 360/1519, Loss: 0.3736463683346907, Mean Spearman (current batch): 0.4548\nBatch 370/1519, Loss: 0.3736972241788297, Mean Spearman (current batch): 0.2670\nBatch 380/1519, Loss: 0.3736527473518723, Mean Spearman (current batch): 0.3915\nBatch 390/1519, Loss: 0.3737171858549118, Mean Spearman (current batch): 0.5260\nBatch 400/1519, Loss: 0.37341143533587456, Mean Spearman (current batch): 0.5431\nBatch 410/1519, Loss: 0.37349200692118667, Mean Spearman (current batch): 0.5904\nBatch 420/1519, Loss: 0.3727378628083638, Mean Spearman (current batch): 0.3130\nBatch 430/1519, Loss: 0.37254221051238307, Mean Spearman (current batch): 0.4802\nBatch 440/1519, Loss: 0.3723738125779412, Mean Spearman (current batch): 0.3757\nBatch 450/1519, Loss: 0.37245783620410494, Mean Spearman (current batch): 0.3358\nBatch 460/1519, Loss: 0.37276788444622705, Mean Spearman (current batch): 0.3419\nBatch 470/1519, Loss: 0.37258382401567824, Mean Spearman (current batch): 0.3084\nBatch 480/1519, Loss: 0.3728449992835522, Mean Spearman (current batch): 0.5502\nBatch 490/1519, Loss: 0.37296782087306585, Mean Spearman (current batch): 0.2311\nBatch 500/1519, Loss: 0.37277726125717164, Mean Spearman (current batch): 0.2328\nBatch 510/1519, Loss: 0.37261125708327575, Mean Spearman (current batch): 0.1613\nBatch 520/1519, Loss: 0.3723620628508238, Mean Spearman (current batch): 0.4364\nBatch 530/1519, Loss: 0.37171992282822447, Mean Spearman (current batch): 0.1945\nBatch 540/1519, Loss: 0.371790474763623, Mean Spearman (current batch): 0.2208\nBatch 550/1519, Loss: 0.3716715716231953, Mean Spearman (current batch): 0.3491\nBatch 560/1519, Loss: 0.37147272367562567, Mean Spearman (current batch): 0.2330\nBatch 570/1519, Loss: 0.37145009825104164, Mean Spearman (current batch): 0.4969\nBatch 580/1519, Loss: 0.3712534101872609, Mean Spearman (current batch): 0.4381\nBatch 590/1519, Loss: 0.3714784558041621, Mean Spearman (current batch): 0.3060\nBatch 600/1519, Loss: 0.37104890038569766, Mean Spearman (current batch): 0.4196\nBatch 610/1519, Loss: 0.37093885233167745, Mean Spearman (current batch): 0.6192\nBatch 620/1519, Loss: 0.37061245984608127, Mean Spearman (current batch): 0.4723\nBatch 630/1519, Loss: 0.37048391836976247, Mean Spearman (current batch): 0.3867\nBatch 640/1519, Loss: 0.3699227014556527, Mean Spearman (current batch): 0.4042\nBatch 650/1519, Loss: 0.36993314009446365, Mean Spearman (current batch): 0.5748\nBatch 660/1519, Loss: 0.36984090746352166, Mean Spearman (current batch): 0.4437\nBatch 670/1519, Loss: 0.3699633379480732, Mean Spearman (current batch): 0.2857\nBatch 680/1519, Loss: 0.36969023675603024, Mean Spearman (current batch): 0.4258\nBatch 690/1519, Loss: 0.36961928472138833, Mean Spearman (current batch): 0.6215\nBatch 700/1519, Loss: 0.36946036049297876, Mean Spearman (current batch): 0.5412\nBatch 710/1519, Loss: 0.3697137555605929, Mean Spearman (current batch): 0.3689\nBatch 720/1519, Loss: 0.3697234823885891, Mean Spearman (current batch): 0.6479\nBatch 730/1519, Loss: 0.3693832410116718, Mean Spearman (current batch): 0.3669\nBatch 740/1519, Loss: 0.36910789697556884, Mean Spearman (current batch): 0.2411\nBatch 750/1519, Loss: 0.36931290276845297, Mean Spearman (current batch): 0.5374\nBatch 760/1519, Loss: 0.36957949250936506, Mean Spearman (current batch): 0.1988\nBatch 770/1519, Loss: 0.3693255082740412, Mean Spearman (current batch): 0.2054\nBatch 780/1519, Loss: 0.3696180313061445, Mean Spearman (current batch): 0.4648\nBatch 790/1519, Loss: 0.3694816013680229, Mean Spearman (current batch): 0.5931\nBatch 800/1519, Loss: 0.36957044001668693, Mean Spearman (current batch): 0.4010\nBatch 810/1519, Loss: 0.36955374280611675, Mean Spearman (current batch): 0.6655\nBatch 820/1519, Loss: 0.3696336609924712, Mean Spearman (current batch): 0.2520\nBatch 830/1519, Loss: 0.36917099981422885, Mean Spearman (current batch): 0.4137\nBatch 840/1519, Loss: 0.36936394912855963, Mean Spearman (current batch): 0.2394\nBatch 850/1519, Loss: 0.3693284929149291, Mean Spearman (current batch): 0.1684\nBatch 860/1519, Loss: 0.36951336805210555, Mean Spearman (current batch): 0.1892\nBatch 870/1519, Loss: 0.3695172019388484, Mean Spearman (current batch): 0.4146\nBatch 880/1519, Loss: 0.36935959790240636, Mean Spearman (current batch): 0.4361\nBatch 890/1519, Loss: 0.3691378133685401, Mean Spearman (current batch): 0.5382\nBatch 900/1519, Loss: 0.36898518459664453, Mean Spearman (current batch): 0.4672\nBatch 910/1519, Loss: 0.3689441426114722, Mean Spearman (current batch): 0.5574\nBatch 920/1519, Loss: 0.3689340137269186, Mean Spearman (current batch): 0.5065\nBatch 930/1519, Loss: 0.3688715247377273, Mean Spearman (current batch): 0.4195\nBatch 940/1519, Loss: 0.3687440172154853, Mean Spearman (current batch): 0.6615\nBatch 950/1519, Loss: 0.36858497563161347, Mean Spearman (current batch): 0.5254\nBatch 960/1519, Loss: 0.3684750203043222, Mean Spearman (current batch): 0.4466\nBatch 970/1519, Loss: 0.36833592158617434, Mean Spearman (current batch): 0.2694\nBatch 980/1519, Loss: 0.3684246660799396, Mean Spearman (current batch): 0.3943\nBatch 990/1519, Loss: 0.36839548042326264, Mean Spearman (current batch): 0.5697\nBatch 1000/1519, Loss: 0.36832885336875915, Mean Spearman (current batch): 0.4194\nBatch 1010/1519, Loss: 0.3681463460520943, Mean Spearman (current batch): 0.5842\nBatch 1020/1519, Loss: 0.3679908239373974, Mean Spearman (current batch): 0.2247\nBatch 1030/1519, Loss: 0.3680867894760613, Mean Spearman (current batch): -0.0341\nBatch 1040/1519, Loss: 0.3681240332241242, Mean Spearman (current batch): 0.3599\nBatch 1050/1519, Loss: 0.36803225710278464, Mean Spearman (current batch): 0.3604\nBatch 1060/1519, Loss: 0.36810159899716105, Mean Spearman (current batch): 0.3372\nBatch 1070/1519, Loss: 0.36814440096093115, Mean Spearman (current batch): 0.4336\nBatch 1080/1519, Loss: 0.36806147796688254, Mean Spearman (current batch): 0.3370\nBatch 1090/1519, Loss: 0.3680882987899518, Mean Spearman (current batch): 0.4048\nBatch 1100/1519, Loss: 0.3682400745153427, Mean Spearman (current batch): 0.4935\nBatch 1110/1519, Loss: 0.3681952939914154, Mean Spearman (current batch): 0.4662\nBatch 1120/1519, Loss: 0.36810752754764897, Mean Spearman (current batch): 0.2886\nBatch 1130/1519, Loss: 0.3679586209027113, Mean Spearman (current batch): 0.3972\nBatch 1140/1519, Loss: 0.3678918519302418, Mean Spearman (current batch): 0.3688\nBatch 1150/1519, Loss: 0.36763576600862585, Mean Spearman (current batch): 0.3980\nBatch 1160/1519, Loss: 0.3676454898355336, Mean Spearman (current batch): 0.2685\nBatch 1170/1519, Loss: 0.367622227826689, Mean Spearman (current batch): 0.4714\nBatch 1180/1519, Loss: 0.3674567848443985, Mean Spearman (current batch): 0.6230\nBatch 1190/1519, Loss: 0.36741793125617406, Mean Spearman (current batch): 0.1960\nBatch 1200/1519, Loss: 0.3672451316813628, Mean Spearman (current batch): 0.3405\nBatch 1210/1519, Loss: 0.3671711532537602, Mean Spearman (current batch): 0.4219\nBatch 1220/1519, Loss: 0.3669807855223046, Mean Spearman (current batch): 0.3190\nBatch 1230/1519, Loss: 0.3670862213382876, Mean Spearman (current batch): 0.5898\nBatch 1240/1519, Loss: 0.36705956860415395, Mean Spearman (current batch): 0.2325\nBatch 1250/1519, Loss: 0.3671153916358948, Mean Spearman (current batch): 0.5209\nBatch 1260/1519, Loss: 0.36700872263265033, Mean Spearman (current batch): 0.2254\nBatch 1270/1519, Loss: 0.3668140650030196, Mean Spearman (current batch): 0.3289\nBatch 1280/1519, Loss: 0.3668475414859131, Mean Spearman (current batch): 0.4227\nBatch 1290/1519, Loss: 0.3668562862762185, Mean Spearman (current batch): 0.6110\nBatch 1300/1519, Loss: 0.36672644324027576, Mean Spearman (current batch): 0.5100\nBatch 1310/1519, Loss: 0.36658321644058667, Mean Spearman (current batch): 0.5030\nBatch 1320/1519, Loss: 0.3665789053069823, Mean Spearman (current batch): 0.4700\nBatch 1330/1519, Loss: 0.3664704600223025, Mean Spearman (current batch): 0.1376\nBatch 1340/1519, Loss: 0.36659022360595306, Mean Spearman (current batch): 0.5593\nBatch 1350/1519, Loss: 0.3665941655635834, Mean Spearman (current batch): 0.4020\nBatch 1360/1519, Loss: 0.36648772786207057, Mean Spearman (current batch): 0.2408\nBatch 1370/1519, Loss: 0.36626510056701017, Mean Spearman (current batch): 0.5483\nBatch 1380/1519, Loss: 0.36627632373053093, Mean Spearman (current batch): 0.2776\nBatch 1390/1519, Loss: 0.3661742467888825, Mean Spearman (current batch): 0.4173\nBatch 1400/1519, Loss: 0.3661133049215589, Mean Spearman (current batch): 0.4322\nBatch 1410/1519, Loss: 0.36599498507401623, Mean Spearman (current batch): 0.3130\nBatch 1420/1519, Loss: 0.36586792492110964, Mean Spearman (current batch): 0.2816\nBatch 1430/1519, Loss: 0.3660619372879709, Mean Spearman (current batch): 0.1430\nBatch 1440/1519, Loss: 0.36586803967754045, Mean Spearman (current batch): 0.4728\nBatch 1450/1519, Loss: 0.3659674276565683, Mean Spearman (current batch): 0.6437\nBatch 1460/1519, Loss: 0.36604258275195345, Mean Spearman (current batch): 0.2934\nBatch 1470/1519, Loss: 0.36592170231196347, Mean Spearman (current batch): 0.5080\nBatch 1480/1519, Loss: 0.36593233242228224, Mean Spearman (current batch): 0.4271\nBatch 1490/1519, Loss: 0.36591959767693644, Mean Spearman (current batch): 0.6718\nBatch 1500/1519, Loss: 0.36596372882525124, Mean Spearman (current batch): 0.1606\nBatch 1510/1519, Loss: 0.3659335744302004, Mean Spearman (current batch): 0.3427\nBatch 1520/1519, Loss: 0.3658484962033598, Mean Spearman (current batch): 0.6273\nEpoch 3/3, Loss: 556.0897142291069, Overall Mean Spearman: 0.3992\ndone\n","output_type":"stream"}]},{"cell_type":"code","source":"# import pandas as pd\n# import torch\n\n# # Load test data\n# # test_df = pd.read_csv('/kaggle/input/google-quest-challenge/test.csv')\n\n# # Preprocess test data\n# def preprocess_data(df):\n#     inputs1 = tokenizer(\n#         df['question_title'].tolist(), \n#         df['question_body'].tolist(), \n#         return_tensors='pt', \n#         padding=True, \n#         truncation=True, \n#         max_length=512\n#     )\n#     inputs2 = tokenizer(\n#         df['question_title'].tolist(), \n#         df['answer'].tolist(), \n#         return_tensors='pt', \n#         padding=True, \n#         truncation=True, \n#         max_length=512\n#     )\n#     inputs3 = tokenizer(\n#         df['question_body'].tolist(), \n#         df['answer'].tolist(), \n#         return_tensors='pt', \n#         padding=True, \n#         truncation=True, \n#         max_length=512\n#     )\n    \n#     return inputs1, inputs2, inputs3\n\n# inputs1, inputs2, inputs3 = preprocess_data(test_df)\n\n# # Make predictions\n# def predict(model, inputs1, inputs2, inputs3):\n#     model.eval()\n#     with torch.no_grad():\n#         input_ids1, attention_mask1 = inputs1['input_ids'], inputs1['attention_mask']\n#         input_ids2, attention_mask2 = inputs2['input_ids'], inputs2['attention_mask']\n#         input_ids3, attention_mask3 = inputs3['input_ids'], inputs3['attention_mask']\n        \n#         pred1, pred2, pred3, final_pred = model(input_ids1, attention_mask1, input_ids2, attention_mask2, input_ids3, attention_mask3)\n        \n#     return final_pred\n\n# predictions = predict(model, inputs1, inputs2, inputs3)\n\n# # Format predictions for submission\n# qa_ids = test_df['qa_id']\n# target_cols = [\n#     'question_asker_intent_understanding', 'question_body_critical', 'question_conversational',\n#     'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer',\n#     'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent',\n#     'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice',\n#     'question_type_compare', 'question_type_consequence', 'question_type_definition',\n#     'question_type_entity', 'question_type_instructions', 'question_type_procedure',\n#     'question_type_reason_explanation', 'question_type_spelling', 'question_well_written',\n#     'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n#     'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure',\n#     'answer_type_reason_explanation', 'answer_well_written'\n# ]\n\n# submission_df = pd.DataFrame(predictions.cpu().numpy(), columns=target_cols)\n# submission_df.insert(0, 'qa_id', qa_ids)\n\n# # Save to CSV\n# submission_df.to_csv('submission.csv', index=False)\n# print(\"Done\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport torch\n\n# Load test data\n# test_df = pd.read_csv('/kaggle/input/google-quest-challenge/test.csv')\n\n# Preprocess test data\ndef preprocess_data(df):\n    inputs1 = tokenizer(\n        df['question_title'].tolist(), \n        df['question_body'].tolist(), \n        return_tensors='pt', \n        padding=True, \n        truncation=True, \n        max_length=512\n    )\n    inputs2 = tokenizer(\n        df['question_title'].tolist(), \n        df['answer'].tolist(), \n        return_tensors='pt', \n        padding=True, \n        truncation=True, \n        max_length=512\n    )\n    inputs3 = tokenizer(\n        df['question_body'].tolist(), \n        df['answer'].tolist(), \n        return_tensors='pt', \n        padding=True, \n        truncation=True, \n        max_length=512\n    )\n    \n    return inputs1, inputs2, inputs3\n\ninputs1, inputs2, inputs3 = preprocess_data(test_df)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T10:07:23.855996Z","iopub.execute_input":"2024-07-02T10:07:23.856396Z","iopub.status.idle":"2024-07-02T10:07:31.471321Z","shell.execute_reply.started":"2024-07-02T10:07:23.856348Z","shell.execute_reply":"2024-07-02T10:07:31.470274Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Process data in chunks to avoid out of memory errors\ndef predict_in_chunks(model, inputs1, inputs2, inputs3, batch_size=4):\n    model.eval()\n    all_preds = []\n    with torch.no_grad():\n        for start_idx in range(0, len(inputs1['input_ids']), batch_size):\n            end_idx = start_idx + batch_size\n            input_ids1, attention_mask1 = inputs1['input_ids'][start_idx:end_idx].to(device), inputs1['attention_mask'][start_idx:end_idx].to(device)\n            input_ids2, attention_mask2 = inputs2['input_ids'][start_idx:end_idx].to(device), inputs2['attention_mask'][start_idx:end_idx].to(device)\n            input_ids3, attention_mask3 = inputs3['input_ids'][start_idx:end_idx].to(device), inputs3['attention_mask'][start_idx:end_idx].to(device)\n            \n            final_pred = model(input_ids1, attention_mask1, input_ids2, attention_mask2, input_ids3, attention_mask3)\n            all_preds.append(final_pred.cpu())\n            \n    return torch.cat(all_preds, dim=0)\n\n# inputs1, inputs2, inputs3 = preprocess_data(test_df)\npredictions = predict_in_chunks(model, inputs1, inputs2, inputs3)\n\n# Format predictions for submission\nqa_ids = test_df['qa_id']\nsubmission_df = pd.DataFrame(predictions.numpy(), columns=target_cols)\nsubmission_df.insert(0, 'qa_id', qa_ids)\n\n# Save to CSV\nsubmission_df.to_csv('submission.csv', index=False)\nprint(\"Done\")","metadata":{"execution":{"iopub.status.busy":"2024-07-02T10:07:31.473363Z","iopub.execute_input":"2024-07-02T10:07:31.473657Z","iopub.status.idle":"2024-07-02T10:07:43.678773Z","shell.execute_reply.started":"2024-07-02T10:07:31.473630Z","shell.execute_reply":"2024-07-02T10:07:43.677407Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"Done\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}